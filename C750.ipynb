{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auditing and Cleaning Open Street Map using Python and Data Wrangling Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose the City of West Linn, Oregon for this project. West Linn is my hometown where I've recided for the first two decades of my life. As such, I am very familiar with its streets and layouts. I've always been a fan of the open source community so I figured this would be a perfect way to contribute back.\n",
    "\n",
    "Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The map in question can be found here: https://overpass-api.de/api/map?bbox=-122.6977,45.2379,-122.4220,45.4584\n",
    "\n",
    "The map file was downloaded using the Overpass API. The full file is approximately 400 MB in size. The contents of the full file is what we will be working with here. In my GitHub is a sample file that you can run yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing I wanted to do was get a grasp of the file that we're working with. How many node tags? Ways tags? And so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'note': 1, 'meta': 1, 'bounds': 1, 'node': 1643082, 'tag': 1180922, 'nd': 1898598, 'way': 223390, 'member': 50651, 'relation': 1998, 'osm': 1}\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "\n",
    "osmfile = \"wlinn\"\n",
    "\n",
    "def tag_count(filename):\n",
    "    \n",
    "    \"\"\"\n",
    "    Counts how many tags are within the XML file and returns it as a dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    tag_dictionary = {}\n",
    "    for event, elem in ET.iterparse(filename):\n",
    "        if elem.tag not in tag_dictionary:\n",
    "            tag_dictionary[elem.tag] = 1\n",
    "        else:\n",
    "            tag_dictionary[elem.tag] += 1\n",
    "            \n",
    "    return tag_dictionary\n",
    "\n",
    "print (tag_count(osmfile))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a summary of what our function outputs:\n",
    "\n",
    "- 134,382 nodes\n",
    "- 223,390 ways\n",
    "- 1,998 relations\n",
    "- 50,651 members\n",
    "- 1,898 nds\n",
    "\n",
    "That's quite a lot to work with. But it's definitely not impossible. Let's discover which values in the tags will give us issues. Not all characters can be imported easily into a database and having consistency will enhance readability. It will be much easier later on to have consistency in our data. We are going to be using regular expresssions (REGEX) in Python. This is what we want to identify in our OSM File:\n",
    "\n",
    "- Values with only lower case letters\n",
    "- Values with only upper case letters\n",
    "- Values that have characters that you wouldn't expect to be in a map\n",
    "    \n",
    "Let's handle this in the function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lowercase': 617224, 'lower_colon': 557043, 'uppercase': 965, 'upper_colon': 2876, 'problem': 0, 'other': 2814}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "lowercase = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "uppercase = re.compile(r'^([A-Z]|_)*$')\n",
    "upper_colon = re.compile(r'^([A-Z]|_)*:([a-z]|_)*$')\n",
    "problem = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "\"\"\"\n",
    "    This funtion will create a dictionary telling us how many entries in the\n",
    "    dataset contain all lower case for \"k=\" values, all uppercase, if the \"k=\"\n",
    "    value contains any problem characters, values with either all lower or all\n",
    "    upper with at least 1 colon, or if there are any characters not\n",
    "    convered in the REGEX\n",
    "\"\"\"\n",
    "\n",
    "def k_type(element, key):\n",
    "            \n",
    "        if element.tag == 'tag': #find only elements named tag\n",
    "            \n",
    "            if lowercase.search(element.attrib['k']): #finds the \"k\" value in the tag\n",
    "                key['lowercase'] +=1\n",
    "            elif lower_colon.search(element.attrib['k']):\n",
    "                key['lower_colon'] +=1\n",
    "            elif uppercase.search(element.attrib['k']):\n",
    "                key['uppercase'] +=1\n",
    "            elif upper_colon.search(element.attrib['k']):\n",
    "                key['upper_colon'] +=1\n",
    "            elif problem.search(element.attrib['k']):\n",
    "                key['problem'] += 1\n",
    "            else:\n",
    "                key['other'] += 1\n",
    "        return key\n",
    "\n",
    "\n",
    "\"\"\"This fuction will parse through an XML file (the OSM file) and will\n",
    "    execute the above function to count the different types of k values \n",
    "    that we have.\n",
    "\"\"\"\n",
    "\n",
    "def process_tag(filename):\n",
    "    \n",
    "    # sets the key variable with 0 in all indexes\n",
    "    key = {\"lowercase\": 0, \"lower_colon\": 0, \"uppercase\": 0, \"upper_colon\": 0, \"problem\": 0, \"other\": 0} \n",
    "    \n",
    "    for _, element in ET.iterparse(filename):\n",
    "        key = k_type(element, key)\n",
    "        \n",
    "    return key\n",
    "\n",
    "tag_dictionary = process_tag(osmfile)\n",
    "print (tag_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thankfully we haven't assesed anything that I would consider a problem in this dataset. However, we do have 2876 tags that do fit the \"uppercase\" description. Let's make a function that takes the dataset as an input and have it output the tags' k value associated with it. We will see what corresponds with the \"uppercase\" that our above function has identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NHS', 'NHS', 'NHS', 'NHS', 'NHS', 'NHS', 'NHS', 'NHS', 'NHS', 'NHS']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_key_with_issues(filename):\n",
    "    \n",
    "    \n",
    "    #takes filename and returns a list of identified issues\n",
    "\n",
    "    issue_list = []\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        if element.tag == 'tag':\n",
    "            if uppercase.search(element.attrib['k']):\n",
    "                issue_list.append(element.attrib['k'])\n",
    "    return issue_list\n",
    "\n",
    "list = get_key_with_issues(osmfile)\n",
    "list[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I truncated the list with list[:10], but there are a lot more results. This is a lot of NHS. Upon some further research, the \"NHS\" value was put in by one overzealous user named Peter Dobratz in 2016. NHS in this case stands for \"National Highway System.\" This is acceptable to have and doesn't necessarily make sense to spell out the acronym in all 2876 cases. It seems our script has caught an erroneous error. I will leave this data untouched."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and Auditing the Data\n",
    "\n",
    "We will analyze the street types that are in this dataset and try to get an angle on how we want to find issues that arise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'Court': 13102, 'Road': 17588, 'Street': 26807, 'Drive': 19373, 'Rd': 7, 'Way': 4571, 'Boulevard': 2795, 'Lane': 6922, 'Avenue': 24919, 'East': 42, 'Circle': 2026, 'Highway': 504, 'Place': 3002, 'West': 72, 'Loop': 1431, 'Terrace': 1362, 'Alley': 2, '213': 107, 'Cervantes': 53, 'Summit': 25, 'Circus': 30, '212': 120, '224': 48, 'Parkway': 367, '97266': 1, 'Ave': 2, 'North': 44, 'Landing': 9, 'Botticelli': 7, 'Touchstone': 55, 'Point': 15, 'South': 47, '99E': 41, 'Vista': 4, 'Wheatland': 4, 'Run': 21, 'Crest': 42, 'Pointe': 2, 'Trail': 170, 'Grotto': 4, 'Downs': 29, 'Polonius': 5, 'Falstaff': 12, 'Pimlico': 4, 'Wheatherstone': 2, 'Woods': 15, 'Hotspur': 12, 'Greco': 1, 'Curve': 11, 'Path': 13, 'Miami': 17, 'Northbound': 1, 'Southbound': 1, 'Spinosa': 20, 'Pericles': 6, 'Commons': 37, 'View': 27, 'Fieldcrest': 46, 'TRL': 2}) %s: %d\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'Court': 26204,\n",
       "             'Road': 35176,\n",
       "             'Street': 53614,\n",
       "             'Drive': 38746,\n",
       "             'Rd': 14,\n",
       "             'Way': 9142,\n",
       "             'Boulevard': 5590,\n",
       "             'Lane': 13844,\n",
       "             'Avenue': 49838,\n",
       "             'East': 84,\n",
       "             'Circle': 4052,\n",
       "             'Highway': 1008,\n",
       "             'Place': 6004,\n",
       "             'West': 144,\n",
       "             'Loop': 2862,\n",
       "             'Terrace': 2724,\n",
       "             'Alley': 4,\n",
       "             '213': 214,\n",
       "             'Cervantes': 106,\n",
       "             'Summit': 50,\n",
       "             'Circus': 60,\n",
       "             '212': 240,\n",
       "             '224': 96,\n",
       "             'Parkway': 734,\n",
       "             '97266': 2,\n",
       "             'Ave': 4,\n",
       "             'North': 88,\n",
       "             'Landing': 18,\n",
       "             'Botticelli': 14,\n",
       "             'Touchstone': 110,\n",
       "             'Point': 30,\n",
       "             'South': 94,\n",
       "             '99E': 82,\n",
       "             'Vista': 8,\n",
       "             'Wheatland': 8,\n",
       "             'Run': 42,\n",
       "             'Crest': 84,\n",
       "             'Pointe': 4,\n",
       "             'Trail': 340,\n",
       "             'Grotto': 8,\n",
       "             'Downs': 58,\n",
       "             'Polonius': 10,\n",
       "             'Falstaff': 24,\n",
       "             'Pimlico': 8,\n",
       "             'Wheatherstone': 4,\n",
       "             'Woods': 30,\n",
       "             'Hotspur': 24,\n",
       "             'Greco': 2,\n",
       "             'Curve': 22,\n",
       "             'Path': 26,\n",
       "             'Miami': 34,\n",
       "             'Northbound': 2,\n",
       "             'Southbound': 2,\n",
       "             'Spinosa': 40,\n",
       "             'Pericles': 12,\n",
       "             'Commons': 74,\n",
       "             'View': 54,\n",
       "             'Fieldcrest': 92,\n",
       "             'TRL': 4})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from audit import *\n",
    "\n",
    "audit(osmfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are quite a few issues here. Using the above output, we have a lot of work to do.\n",
    "\n",
    "- Roads are erroneously labeled as \"Rd\" and \"Ave\" should be spelt out.\n",
    "- 97266 is an area/zip code and should not be in the street name field.\n",
    "- Pimlico is the name of a Drive in my hometown and should be appended with \"Drive\" to aid in consistency.\n",
    "- The value \"TRL\" should be spelt out fully as \"Trail\" to aid in consistency as well.\n",
    "- Boticelli is a street.\n",
    "- 97266 is a ZIP code and should not be in this field. The location refers to a delicious Mexican restaurant. The v field in the tag element is \"8202 SE Flavel St, Portland, OR 97266\". It should just be \"SE Flavel Street\".\n",
    "- Wheatland is a road.\n",
    "- Falstaff is a road.\n",
    "- Pimlico is a Drive.\n",
    "- Hotspur is a road.\n",
    "- Southwest Miami is a street.\n",
    "- Pericles is a loop.\n",
    "- Polonius is a loop.\n",
    "- El Greco is a street.\n",
    "- Wheatherstone is a street\n",
    "- View, Commons, Run, South, North, Circus, Summit, Downs, West, View, and East are all acceptable values. I will add them to the expected_values dictionary.\n",
    "- Cervantes is a street.\n",
    "- Touchstone is a road.\n",
    "- Polonius is a street.\n",
    "- Spinosa is a road.\n",
    "- Southeast Fieldcrest is a road.\n",
    "\n",
    "Based on the above findings, I've created a dictionary that will map an incorrect value to a correct value and a list of abbreviations with their corrected value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Values:\n",
      "['Avenue', 'Alley', 'Road', 'Street', 'Trail', 'Landing', 'Pointe', 'Vista', 'Woods', 'Curve', 'Path', 'Freeway', 'Grotto', 'Court', 'Northbound', 'Southbound', 'Drive', 'Boulevard', 'Lane', 'Circle', 'Highway', 'Place', 'Loop', 'Terrace', 'Way', 'Crest', 'Parkway', 'Point', 'View', 'Commons', 'Run', 'South', 'North', 'East', 'Circus', 'Summit', 'West', '99E', '224', '213', 'View', '212', 'Downs']\n",
      "\n",
      "Abbreivation Mappings:\n",
      "{'Ave': 'Avenue', 'TRL': 'Trail', 'Hwy': 'Highway', 'Rd': 'Road', 'Ct': 'Court', 'Dr': 'Drive', 'Pl': 'Place', 'place': 'Place', 'Pkwy': 'Parkway', 'rd.': 'Road', 'Sq.': 'Square', 'St': 'Street', 'st': 'Street', 'ST': 'Street', 'St,': 'Street', 'St.': 'Street', 'street': 'Street', 'Street.': 'Street'}\n"
     ]
    }
   ],
   "source": [
    "from map_cleaning import *\n",
    "\n",
    "print(\"Expected Values:\")\n",
    "print(expected_values)\n",
    "print()\n",
    "print(\"Abbreivation Mappings:\")\n",
    "print(abbr_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have expected values for streets and abbreviations taken care of, let's fix specific streets that won't be fixed by the above two dictionaries. Here is the spelling_fix dictionary used to fix specific issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Falstaff': 'Falstaff Road', 'Pimlico': 'Pimlico Drive', 'Hotspur': 'Hotspur Road', 'Pericles': 'Pericles Loop', 'El Greco': 'El Greco Street', '8202 SE Flavel St, Portland, OR 97266': 'SE Flavel Street', 'Cervantes': 'Cervantes Street', 'Touchstone': 'Touchstone Road', 'Polonius': 'Polonius Street', 'Spinosa': 'Spinosa Road', 'Boticelli': 'Boticelli Street', 'Southwest Wheatland': 'Southwest Wheatland Road', 'Southwest Miami': 'Southwest Miami Street', 'Wheatherstone': 'Wheatherstone Street', 'Southeast Fieldcrest': 'Southeast Fieldcrest Road'}\n"
     ]
    }
   ],
   "source": [
    "print(spelling_fix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now We Will Clean The Data!\n",
    "##### (This is the best part!)\n",
    "\n",
    "With the above out of the way, let's clean the map so that we can put our above dictionaries to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4th Ave: 4th Avenue\n",
      "7273 SE 92nd Ave: 7273 SE 92nd Avenue\n",
      "Cervantes: Cervantes Street\n",
      "Falstaff: Falstaff Road\n",
      "Hotspur: Hostspur Road\n",
      "Pericles: Pericles Loop\n",
      "Pimlico: Pimlico Drive\n",
      "Polonius: Polonius Loop\n",
      "S Carus Rd: S Carus Road\n",
      "S Penman Rd: S Penman Road\n",
      "SE Stevens Rd: SE Stevens Road\n",
      "SE Sunnyside Rd: SE Sunnyside Road\n",
      "Southeast Hittay TRL: Southeast Hittay Trail\n",
      "Southwest Borland Rd: Southwest Borland Road\n",
      "Spinosa: Spinosa Road\n",
      "SW Boones Ferry Rd: SW Boones Ferry Road\n",
      "Touchstone: Touchstone Road\n"
     ]
    }
   ],
   "source": [
    "from map_cleaning import *\n",
    "\n",
    "clean_map(osmfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has now been cleaned. Let's create our CSV files, create our database with tables, and import our CSV files into our newly created database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from create_csvs import *\n",
    "from createdb import *\n",
    "\n",
    "process_map(osmfile, validate=False)\n",
    "create_db()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executing Queries in the Database\n",
    "\n",
    "Here I will analyze the data to find the answers to a few questions I had."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Who are the top contributing users? And how many contributions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Peter Dobratz_pdxbuildings', 615824), ('lyzidiamond_imports', 218977), ('Peter Dobratz', 101784), ('justin_pdxbuildings', 95643), ('Mele Sax-Barnett', 87759), ('Darrell_pdxbuildings', 86402), ('Grant Humphries', 73924), ('baradam', 41090), ('cowdog', 25071), ('tguen', 23761)]\n"
     ]
    }
   ],
   "source": [
    "#Creates connection to the databse\n",
    "conn = sqlite3.connect(sqlite_file)\n",
    "c = conn.cursor()\n",
    "    \n",
    "QUERY = '''\n",
    "SELECT DISTINCT user, COUNT(*)\n",
    "FROM nodes\n",
    "GROUP BY nodes.uid\n",
    "ORDER BY COUNT(*) DESC\n",
    "LIMIT 10;\n",
    "'''\n",
    "\n",
    "c.execute(QUERY)\n",
    "all_rows = c.fetchall()\n",
    "print(all_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What's the most common way tag?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('building', 164482), ('ele', 141702), ('street', 120624), ('housenumber', 120525), ('city', 120378), ('postcode', 120275), ('height', 72492), ('highway', 50043), ('levels', 27997), ('name', 24671)]\n"
     ]
    }
   ],
   "source": [
    "#Creates connection to the databse\n",
    "conn = sqlite3.connect(sqlite_file)\n",
    "c = conn.cursor()\n",
    "\n",
    "QUERY = '''\n",
    "SELECT key,count(*)\n",
    "FROM ways_tags\n",
    "GROUP BY 1\n",
    "ORDER BY count(*) DESC\n",
    "LIMIT 10;\n",
    "'''\n",
    "\n",
    "c.execute(QUERY)\n",
    "all_rows = c.fetchall()\n",
    "print(all_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And the most common node tag?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('highway', 9152), ('street', 5321), ('housenumber', 5318), ('city', 5265), ('postcode', 5263), ('name', 3462), ('barrier', 2663), ('ref', 1790), ('public_transport', 1687), ('amenity', 1537)]\n"
     ]
    }
   ],
   "source": [
    "#Creates connection to the databse\n",
    "conn = sqlite3.connect(sqlite_file)\n",
    "c = conn.cursor()\n",
    "\n",
    "QUERY = '''\n",
    "SELECT key,count(*)\n",
    "FROM nodes_tags\n",
    "GROUP BY 1\n",
    "ORDER BY count(*) DESC\n",
    "LIMIT 10;\n",
    "'''\n",
    "c.execute(QUERY)\n",
    "all_rows = c.fetchall()\n",
    "print(all_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What about the most common ammenities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('bench', 176), ('bicycle_parking', 137), ('place_of_worship', 104), ('restaurant', 79), ('parking_space', 78), ('waste_basket', 67), ('post_box', 66), ('fast_food', 62), ('cafe', 56), ('parking', 55), ('doctors', 50), ('pharmacy', 43), ('toilets', 36), ('school', 35), ('dentist', 35), ('drinking_water', 34), ('letter_box', 32), ('fuel', 32), ('bank', 28), ('social_facility', 25), ('atm', 23), ('vending_machine', 22), ('public_bookcase', 19), ('pub', 18), ('parking_entrance', 18)]\n"
     ]
    }
   ],
   "source": [
    "#Creates connection to the databse\n",
    "conn = sqlite3.connect(sqlite_file)\n",
    "c = conn.cursor()\n",
    "\n",
    "QUERY = '''\n",
    "SELECT value, COUNT(*) as Count\n",
    "FROM nodes_tags\n",
    "WHERE key='amenity'\n",
    "GROUP BY value\n",
    "ORDER BY Count DESC\n",
    "LIMIT 25;\n",
    "'''\n",
    "\n",
    "c.execute(QUERY)\n",
    "all_rows = c.fetchall()\n",
    "print(all_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I see that we have 32 \"fuel\" locations. Let's look deeper. How many of them offer diesel fuel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('yes', 8)]\n"
     ]
    }
   ],
   "source": [
    "#Creates connection to the databse\n",
    "conn = sqlite3.connect(sqlite_file)\n",
    "c = conn.cursor()\n",
    "\n",
    "QUERY = '''\n",
    "SELECT nodes_tags.value, COUNT(*) as Count\n",
    "FROM nodes_tags \n",
    "JOIN\n",
    "    (SELECT DISTINCT(id)\n",
    "    FROM nodes_tags\n",
    "    WHERE value='fuel') as f\n",
    "ON nodes_tags.id=f.id\n",
    "WHERE nodes_tags.key='diesel'\n",
    "GROUP BY nodes_tags.value\n",
    "ORDER BY Count DESC;\n",
    "'''\n",
    "\n",
    "c.execute(QUERY)\n",
    "all_rows = c.fetchall()\n",
    "print(all_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's investigate the most common place of worship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('christian', 97), ('buddhist', 3), ('unitarian_universalist', 1), ('jewish', 1)]\n"
     ]
    }
   ],
   "source": [
    "#Creates connection to the databse\n",
    "conn = sqlite3.connect(sqlite_file)\n",
    "c = conn.cursor()\n",
    "\n",
    "QUERY = '''\n",
    "SELECT nodes_tags.value, COUNT(*) as Count\n",
    "FROM nodes_tags \n",
    "JOIN\n",
    "    (SELECT DISTINCT(id)\n",
    "    FROM nodes_tags\n",
    "    WHERE value='place_of_worship') as Sub\n",
    "ON nodes_tags.id=Sub.id\n",
    "WHERE nodes_tags.key='religion'\n",
    "GROUP BY nodes_tags.value\n",
    "ORDER BY Count DESC;\n",
    "'''\n",
    "\n",
    "c.execute(QUERY)\n",
    "all_rows = c.fetchall()\n",
    "print(all_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The area apperas overwhelmingly Christian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "### Closing Remarks\n",
    "At first, I focused on cleaning up minor issues. I have lived in the West Linn area for two decades of my life so I am happy to be able to make changes to a database for an area I love.\n",
    "\n",
    "Some common issues in the database is a lack of conformity in naming conventions and data entry. My guess is that OSM keeps the requirements of entry loose in order to decrease the barriers to entry. This allows OSM to obtain a much larger database. If OSM were to tighten the requirements for their database, I have a feeling that the amount of entries would be much less.\n",
    "\n",
    "OSM is in a sweet spot because they are maintained by a supportive community. Other schools throughout the United States have programs wheeree students cleanup the database as well. It's a win-win situation; the database grows almost unhindered and it gets cleaned in the pursuit of knowledge.\n",
    "\n",
    "### Benefits\n",
    "\n",
    "In the future, I would like to expand the maps size and possible cleanup a metropolitan area. To do so, however, I would need to refine my modifications to ensure accuracy. Additionally, I would like to clean up business and increase uniformity of businesses. For example, making sure that phone numbers are in a consistent format or ensuring proper capitalization of business names. Finally, benefits from the above suggestion would include contributing to an opensource community and \n",
    "\n",
    "### Anticipated Problems \n",
    "\n",
    "Some issues with the above suggestions would include non-standard business names. For example, businesses that are intentionally named with all upper or lower case letters. Knowledge of the specific businesses would be key to making sure that incorrect modifications aren't made. Also, finding and correcting phone numbers would be a challenge. The first step would be to find businesses with incorrect phone numbers. The next step would be to query another database with known good numbers. And finally, the third step would be to make modifications. This could be time consuming and potentially bad for the business if an incorrect number is inserted so care and caution should be exercised before making large updates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "https://hadrien-lcrx.github.io/notebooks/Boston_Data_Wrangling.html\n",
    "https://www.w3schools.com/sql/sql_syntax.asp\n",
    "https://wiki.python.org/moin/BeginnersGuide\n",
    "https://github.com/ian-whitestone/data-wrangling-openstreetmap"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
