{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets's first get an idea of the size of the data set. The file is neaerly 400 MB in size. Let's find out how many of each tags exist within the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'note': 1, 'meta': 1, 'bounds': 1, 'node': 1643082, 'tag': 1180922, 'nd': 1898598, 'way': 223390, 'member': 50651, 'relation': 1998, 'osm': 1}\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "\n",
    "osmfile = \"wlinn\"\n",
    "\n",
    "def tag_count(filename):\n",
    "    \n",
    "    \"\"\"\n",
    "    Counts how many tags are within the XML file and returns it as a dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    tag_dictionary = {}\n",
    "    for event, elem in ET.iterparse(filename):\n",
    "        if elem.tag not in tag_dictionary:\n",
    "            tag_dictionary[elem.tag] = 1\n",
    "        else:\n",
    "            tag_dictionary[elem.tag] += 1\n",
    "            \n",
    "    return tag_dictionary\n",
    "\n",
    "print (tag_count(osmfile))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a summary of what our function outputs:\n",
    "\n",
    "- 134,382 nodes\n",
    "- 223,390 ways\n",
    "- 1,998 relations\n",
    "- 50,651 members\n",
    "- 1,898 nds\n",
    "\n",
    "That's quite a lot to work with. But it's definitely not impossible. Let's continue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's discover which values in the tags will give us issues. Not all characters can be imported easily into a database and having consistency will enhance readability. It will be much easier later on to have consistency in our data. We are going to be using regular expresssions (REGEX) in Python. This is what we want to identify in our OSM File:\n",
    "\n",
    "- Values with only lower case letters\n",
    "- Values with only upper case letters\n",
    "- Values that have characters that you wouldn't expect to be in a map\n",
    "    \n",
    "Let's handle this in the function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lowercase': 617224, 'lower_colon': 557043, 'uppercase': 965, 'upper_colon': 2876, 'problem': 0, 'other': 2814}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "lowercase = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "uppercase = re.compile(r'^([A-Z]|_)*$')\n",
    "upper_colon = re.compile(r'^([A-Z]|_)*:([a-z]|_)*$')\n",
    "problem = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "\"\"\"\n",
    "    This funtion will create a dictionary telling us how many entries in the\n",
    "    dataset contain all lower case for \"k=\" values, all uppercase, if the \"k=\"\n",
    "    value contains any problem characters, values with either all lower or all\n",
    "    upper with at least 1 colon, or if there are any characters not\n",
    "    convered in the REGEX\n",
    "\"\"\"\n",
    "\n",
    "def k_type(element, key):\n",
    "            \n",
    "        if element.tag == 'tag': #find only elements named tag\n",
    "            \n",
    "            if lowercase.search(element.attrib['k']): #finds the \"k\" value in the tag\n",
    "                key['lowercase'] +=1\n",
    "            elif lower_colon.search(element.attrib['k']):\n",
    "                key['lower_colon'] +=1\n",
    "            elif uppercase.search(element.attrib['k']):\n",
    "                key['uppercase'] +=1\n",
    "            elif upper_colon.search(element.attrib['k']):\n",
    "                key['upper_colon'] +=1\n",
    "            elif problem.search(element.attrib['k']):\n",
    "                key['problem'] += 1\n",
    "            else:\n",
    "                key['other'] += 1\n",
    "        return key\n",
    "\n",
    "\n",
    "\"\"\"This fuction will parse through an XML file (the OSM file) and will\n",
    "    execute the above function to count the different types of k values \n",
    "    that we have.\n",
    "\"\"\"\n",
    "\n",
    "def process_tag(filename):\n",
    "    \n",
    "    # sets the key variable with 0 in all indexes\n",
    "    key = {\"lowercase\": 0, \"lower_colon\": 0, \"uppercase\": 0, \"upper_colon\": 0, \"problem\": 0, \"other\": 0} \n",
    "    \n",
    "    for _, element in ET.iterparse(filename):\n",
    "        key = k_type(element, key)\n",
    "        \n",
    "    return key\n",
    "\n",
    "tag_dictionary = process_tag(osmfile)\n",
    "print (tag_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thankfully we haven't assesed anything that I would consider a problem in this dataset. However, we do have 2876 tags that do fit the \"uppercase\" description. Let's make a function that takes the dataset as an input and have it output the tags' k value associated with it. We will see what corresponds with the \"uppercase\" that our above function has identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NHS', 'NHS', 'NHS', 'NHS', 'NHS', 'NHS', 'NHS', 'NHS', 'NHS', 'NHS']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_key_with_issues(filename):\n",
    "    \n",
    "    \n",
    "    #takes filename and returns a list of identified issues\n",
    "\n",
    "    issue_list = []\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        if element.tag == 'tag':\n",
    "            if uppercase.search(element.attrib['k']):\n",
    "                issue_list.append(element.attrib['k'])\n",
    "    return issue_list\n",
    "\n",
    "list = get_key_with_issues(osmfile)\n",
    "list[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I truncated the list with list[:100], but there are a lot more results. This is a lot of NHS. Upon some further research, the \"NHS\" value was put in by one overzealous user named Peter Dobratz in 2016. NHS in this case stands for \"National Highway System.\" This is acceptable to have and doesn't necessarily make sense to spell out the acronym in all 2876 cases. It seems our script has caught an erroneous error. I will leave this data untouched."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and Auditing the Data\n",
    "\n",
    "We will analyze the street types that are in this dataset and try to get an angle on how we want to find issues that arise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'Court': 13102, 'Road': 17588, 'Street': 26807, 'Drive': 19373, 'Rd': 7, 'Way': 4571, 'Boulevard': 2795, 'Lane': 6922, 'Avenue': 24919, 'East': 42, 'Circle': 2026, 'Highway': 504, 'Place': 3002, 'West': 72, 'Loop': 1431, 'Terrace': 1362, 'Alley': 2, '213': 107, 'Cervantes': 53, 'Summit': 25, 'Circus': 30, '212': 120, '224': 48, 'Parkway': 367, '97266': 1, 'Ave': 2, 'North': 44, 'Landing': 9, 'Botticelli': 7, 'Touchstone': 55, 'Point': 15, 'South': 47, '99E': 41, 'Vista': 4, 'Wheatland': 4, 'Run': 21, 'Crest': 42, 'Pointe': 2, 'Trail': 170, 'Grotto': 4, 'Downs': 29, 'Polonius': 5, 'Falstaff': 12, 'Pimlico': 4, 'Wheatherstone': 2, 'Woods': 15, 'Hotspur': 12, 'Greco': 1, 'Curve': 11, 'Path': 13, 'Miami': 17, 'Northbound': 1, 'Southbound': 1, 'Spinosa': 20, 'Pericles': 6, 'Commons': 37, 'View': 27, 'Fieldcrest': 46, 'TRL': 2}) %s: %d\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'Court': 13102,\n",
       "             'Road': 17588,\n",
       "             'Street': 26807,\n",
       "             'Drive': 19373,\n",
       "             'Rd': 7,\n",
       "             'Way': 4571,\n",
       "             'Boulevard': 2795,\n",
       "             'Lane': 6922,\n",
       "             'Avenue': 24919,\n",
       "             'East': 42,\n",
       "             'Circle': 2026,\n",
       "             'Highway': 504,\n",
       "             'Place': 3002,\n",
       "             'West': 72,\n",
       "             'Loop': 1431,\n",
       "             'Terrace': 1362,\n",
       "             'Alley': 2,\n",
       "             '213': 107,\n",
       "             'Cervantes': 53,\n",
       "             'Summit': 25,\n",
       "             'Circus': 30,\n",
       "             '212': 120,\n",
       "             '224': 48,\n",
       "             'Parkway': 367,\n",
       "             '97266': 1,\n",
       "             'Ave': 2,\n",
       "             'North': 44,\n",
       "             'Landing': 9,\n",
       "             'Botticelli': 7,\n",
       "             'Touchstone': 55,\n",
       "             'Point': 15,\n",
       "             'South': 47,\n",
       "             '99E': 41,\n",
       "             'Vista': 4,\n",
       "             'Wheatland': 4,\n",
       "             'Run': 21,\n",
       "             'Crest': 42,\n",
       "             'Pointe': 2,\n",
       "             'Trail': 170,\n",
       "             'Grotto': 4,\n",
       "             'Downs': 29,\n",
       "             'Polonius': 5,\n",
       "             'Falstaff': 12,\n",
       "             'Pimlico': 4,\n",
       "             'Wheatherstone': 2,\n",
       "             'Woods': 15,\n",
       "             'Hotspur': 12,\n",
       "             'Greco': 1,\n",
       "             'Curve': 11,\n",
       "             'Path': 13,\n",
       "             'Miami': 17,\n",
       "             'Northbound': 1,\n",
       "             'Southbound': 1,\n",
       "             'Spinosa': 20,\n",
       "             'Pericles': 6,\n",
       "             'Commons': 37,\n",
       "             'View': 27,\n",
       "             'Fieldcrest': 46,\n",
       "             'TRL': 2})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "#we are using defaultdict incase we access a key that doesn't exist yet\n",
    "\n",
    "street_type_re = re.compile(r'\\S+\\.?$', re.IGNORECASE)\n",
    "street_types = defaultdict(int)\n",
    "\n",
    "def check_street_types(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        street_types[street_type] += 1\n",
    "\n",
    "def print_sorted_dictionary(d, expresssion):\n",
    "    keys = d.keys()\n",
    "    keys = sorted(keys, key=lambda s: s.lower())\n",
    "    for k in keys:\n",
    "        v = d[k]\n",
    "        print (expression % (k, v))\n",
    "        \n",
    "def is_street_name(elem):\n",
    "    return (elem.tag == \"tag\") and (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "def check(filename):\n",
    "    for event, elem in ET.iterparse(filename):\n",
    "        if is_street_name(elem):\n",
    "            check_street_types(street_types, elem.attrib['v'])\n",
    "    print(street_types, \"%s: %d\")\n",
    "    return(street_types)\n",
    "    \n",
    "every_type = check(osmfile)\n",
    "every_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are quite a few issues here. \n",
    "\n",
    "- Roads are erroneously labeled as \"Rd\" and \"Ave\" should be spelt out.\n",
    "- 212, 224, 213, and 99E are all highways in the state of Oregon. They should be labeled a bit differently and not just by their number.\n",
    "- 97266 is an area/zip code and should not be in the street name field.\n",
    "- Pimlico is the name of a street in my hometown and should be appended with \"Street\" to aid in consistency.\n",
    "- The value \"TRL\" should be spelt out fully as \"Trail\" to aid in consistency as well.\n",
    "\n",
    "We will fix these items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'Court': 13102, 'Road': 17588, 'Street': 26807, 'Drive': 19373, 'Rd': 7, 'Way': 4571, 'Boulevard': 2795, 'Lane': 6922, 'Avenue': 24919, 'East': 42, 'Circle': 2026, 'Highway': 504, 'Place': 3002, 'West': 72, 'Loop': 1431, 'Terrace': 1362, 'Alley': 2, '213': 107, 'Cervantes': 53, 'Summit': 25, 'Circus': 30, '212': 120, '224': 48, 'Parkway': 367, '97266': 1, 'Ave': 2, 'North': 44, 'Landing': 9, 'Botticelli': 7, 'Touchstone': 55, 'Point': 15, 'South': 47, '99E': 41, 'Vista': 4, 'Wheatland': 4, 'Run': 21, 'Crest': 42, 'Pointe': 2, 'Trail': 170, 'Grotto': 4, 'Downs': 29, 'Polonius': 5, 'Falstaff': 12, 'Pimlico': 4, 'Wheatherstone': 2, 'Woods': 15, 'Hotspur': 12, 'Greco': 1, 'Curve': 11, 'Path': 13, 'Miami': 17, 'Northbound': 1, 'Southbound': 1, 'Spinosa': 20, 'Pericles': 6, 'Commons': 37, 'View': 27, 'Fieldcrest': 46, 'TRL': 2}) %s: %d\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'Court': 13102,\n",
       "             'Road': 17588,\n",
       "             'Street': 26807,\n",
       "             'Drive': 19373,\n",
       "             'Rd': 7,\n",
       "             'Way': 4571,\n",
       "             'Boulevard': 2795,\n",
       "             'Lane': 6922,\n",
       "             'Avenue': 24919,\n",
       "             'East': 42,\n",
       "             'Circle': 2026,\n",
       "             'Highway': 504,\n",
       "             'Place': 3002,\n",
       "             'West': 72,\n",
       "             'Loop': 1431,\n",
       "             'Terrace': 1362,\n",
       "             'Alley': 2,\n",
       "             '213': 107,\n",
       "             'Cervantes': 53,\n",
       "             'Summit': 25,\n",
       "             'Circus': 30,\n",
       "             '212': 120,\n",
       "             '224': 48,\n",
       "             'Parkway': 367,\n",
       "             '97266': 1,\n",
       "             'Ave': 2,\n",
       "             'North': 44,\n",
       "             'Landing': 9,\n",
       "             'Botticelli': 7,\n",
       "             'Touchstone': 55,\n",
       "             'Point': 15,\n",
       "             'South': 47,\n",
       "             '99E': 41,\n",
       "             'Vista': 4,\n",
       "             'Wheatland': 4,\n",
       "             'Run': 21,\n",
       "             'Crest': 42,\n",
       "             'Pointe': 2,\n",
       "             'Trail': 170,\n",
       "             'Grotto': 4,\n",
       "             'Downs': 29,\n",
       "             'Polonius': 5,\n",
       "             'Falstaff': 12,\n",
       "             'Pimlico': 4,\n",
       "             'Wheatherstone': 2,\n",
       "             'Woods': 15,\n",
       "             'Hotspur': 12,\n",
       "             'Greco': 1,\n",
       "             'Curve': 11,\n",
       "             'Path': 13,\n",
       "             'Miami': 17,\n",
       "             'Northbound': 1,\n",
       "             'Southbound': 1,\n",
       "             'Spinosa': 20,\n",
       "             'Pericles': 6,\n",
       "             'Commons': 37,\n",
       "             'View': 27,\n",
       "             'Fieldcrest': 46,\n",
       "             'TRL': 2})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "street_type_re = re.compile(r'\\S+\\.?$', re.IGNORECASE)\n",
    "street_types = defaultdict(int)\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        street_types[street_type] += 1\n",
    "\n",
    "def print_sorted_dict(d, expression):\n",
    "    keys = d.keys()\n",
    "    keys = sorted(keys, key=lambda s: s.lower())\n",
    "    for k in keys:\n",
    "        v = d[k]\n",
    "        print (expression % (k, v))\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.tag == \"tag\") and (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "def audit(filename):\n",
    "    for event, elem in ET.iterparse(filename):\n",
    "        if is_street_name(elem):\n",
    "            audit_street_type(street_types, elem.attrib['v'])\n",
    "    print(street_types, \"%s: %d\")\n",
    "    return(street_types)\n",
    "\n",
    "all_types = audit(osmfile)\n",
    "all_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1 We will make a dictionary of what we should expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_values = ['Avenue', 'Alley', 'Road', 'Street', 'Trail', 'Landing', 'Pointe', \n",
    "                   'Vista', 'Woods', 'Curve', 'Path', 'Freeway', 'Grotto', 'Court', \n",
    "                   'Northbound', 'Southbound', 'Drive', 'Boulevard', 'Lane', 'Circle',\n",
    "                   'Highway', 'Place', 'Loop', 'Terrace', 'Way', 'Crest', 'Parkway',\n",
    "                   'Point']\n",
    "\n",
    "abbr_mapping = {'Ave': 'Avenue',\n",
    "                'TRL': 'Trail',\n",
    "                'Hwy': 'Highway',\n",
    "                'Rd': 'Road',\n",
    "                'Ave': 'Avenue',\n",
    "                'Ct': 'Court',\n",
    "                'Dr': 'Drive',\n",
    "                'Pl': 'Place',\n",
    "                'place': 'Place',\n",
    "                'Pkwy': 'Parkway',\n",
    "                'rd.': 'Road',\n",
    "                'Sq.': 'Square',\n",
    "                'St': 'Street',\n",
    "                'st': 'Street',\n",
    "                'ST': 'Street',\n",
    "                'St,': 'Street',\n",
    "                'St.': 'Street',\n",
    "                'street': 'Street',\n",
    "                'Street.': 'Street'\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below code, we are only going to be printing values where the type is <20 and that are not yet in expected_values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'97266': ['8202 SE Flavel St, Portland, OR 97266'],\n",
       " 'Botticelli': ['Botticelli',\n",
       "  'Botticelli',\n",
       "  'Botticelli',\n",
       "  'Botticelli',\n",
       "  'Botticelli',\n",
       "  'Botticelli',\n",
       "  'Botticelli'],\n",
       " 'Wheatland': ['Southwest Wheatland',\n",
       "  'Southwest Wheatland',\n",
       "  'Southwest Wheatland',\n",
       "  'Southwest Wheatland'],\n",
       " 'Polonius': ['Polonius', 'Polonius', 'Polonius', 'Polonius', 'Polonius'],\n",
       " 'Falstaff': ['Falstaff',\n",
       "  'Falstaff',\n",
       "  'Falstaff',\n",
       "  'Falstaff',\n",
       "  'Falstaff',\n",
       "  'Falstaff',\n",
       "  'Falstaff',\n",
       "  'Falstaff',\n",
       "  'Falstaff',\n",
       "  'Falstaff',\n",
       "  'Falstaff',\n",
       "  'Falstaff'],\n",
       " 'Pimlico': ['Pimlico', 'Pimlico', 'Pimlico', 'Pimlico'],\n",
       " 'Wheatherstone': ['Wheatherstone', 'Wheatherstone'],\n",
       " 'Hotspur': ['Hotspur',\n",
       "  'Hotspur',\n",
       "  'Hotspur',\n",
       "  'Hotspur',\n",
       "  'Hotspur',\n",
       "  'Hotspur',\n",
       "  'Hotspur',\n",
       "  'Hotspur',\n",
       "  'Hotspur',\n",
       "  'Hotspur',\n",
       "  'Hotspur',\n",
       "  'Hotspur'],\n",
       " 'Greco': ['El Greco'],\n",
       " 'Miami': ['Southwest Miami',\n",
       "  'Southwest Miami',\n",
       "  'Southwest Miami',\n",
       "  'Southwest Miami',\n",
       "  'Southwest Miami',\n",
       "  'Southwest Miami',\n",
       "  'Southwest Miami',\n",
       "  'Southwest Miami',\n",
       "  'Southwest Miami',\n",
       "  'Southwest Miami',\n",
       "  'Southwest Miami',\n",
       "  'Southwest Miami',\n",
       "  'Southwest Miami',\n",
       "  'Southwest Miami',\n",
       "  'Southwest Miami',\n",
       "  'Southwest Miami',\n",
       "  'Southwest Miami'],\n",
       " 'Pericles': ['Pericles',\n",
       "  'Pericles',\n",
       "  'Pericles',\n",
       "  'Pericles',\n",
       "  'Pericles',\n",
       "  'Pericles']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "typo_full_names = {}\n",
    "\n",
    "def audit_street_name(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if (all_types[street_type] < 20) and (street_type not in expected_values) and (street_type not in abbr_mapping):\n",
    "            if street_type in typo_full_names:\n",
    "                typo_full_names[street_type].append(street_name)\n",
    "            else:\n",
    "                typo_full_names.update({ street_type:[street_name] })\n",
    "\n",
    "def audit_name(filename):\n",
    "    for event, elem in ET.iterparse(filename):\n",
    "        if is_street_name(elem):\n",
    "            audit_street_name(street_types, elem.attrib['v'])    \n",
    "    return typo_full_names\n",
    "\n",
    "audit_name(osmfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above output, we have a lot of work to do.\n",
    "\n",
    "1. Boticelli is a street.\n",
    "2. 97266 is a ZIP code and should not be in this field. The location refers to a delicious Mexican restaurant. The v field in the tag element is \"8202 SE Flavel St, Portland, OR 97266\". It should just be \"SE Flavel Street\".\n",
    "3. Wheatland is a road.\n",
    "4. Falstaff is a road.\n",
    "5. Pimlico is a Drive.\n",
    "6. Hotspur is a road.\n",
    "7. Southwest Miami is a street.\n",
    "8. Pericles is a loop.\n",
    "9. Polonius is a loop.\n",
    "10. El Greco is a street.\n",
    "11. Wheatherstone is a street\n",
    "13. View, Commons, Run, South, North, Circus, Summit, Downs, West, View, and East are all acceptable values. I will add them to the expected_values dictionary.\n",
    "14. Upon further inspection, Highways 99E, 213, 212, and 224 are all labeled correctly. They will be added to expected_values as well\n",
    "15. Cervantes is a street.\n",
    "16. Touchstone is a road.\n",
    "17. Polonius is a street.\n",
    "18. Spinosa is a road.\n",
    "19. Southeast Fieldcrest is a road.\n",
    "\n",
    "Let's get to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_values.extend([\n",
    "    'View', 'Commons', 'Run', 'South', 'North', 'East', 'Circus', 'Summit', 'West', \n",
    "    '99E', '224', '213', 'View', '212', 'Downs'\n",
    "                       ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rerun the above script to get a final output of what to fix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'97266': ['8202 SE Flavel St, Portland, OR 97266',\n",
       "  '8202 SE Flavel St, Portland, OR 97266'],\n",
       " 'Botticelli': ['Botticelli',\n",
       "  'Botticelli',\n",
       "  'Botticelli',\n",
       "  'Botticelli',\n",
       "  'Botticelli',\n",
       "  'Botticelli',\n",
       "  'Botticelli',\n",
       "  'Botticelli',\n",
       "  'Botticelli',\n",
       "  'Botticelli',\n",
       "  'Botticelli',\n",
       "  'Botticelli',\n",
       "  'Botticelli',\n",
       "  'Botticelli'],\n",
       " 'Wheatland': ['Southwest Wheatland',\n",
       "  'Southwest Wheatland',\n",
       "  'Southwest Wheatland',\n",
       "  'Southwest Wheatland',\n",
       "  'Southwest Wheatland',\n",
       "  'Southwest Wheatland',\n",
       "  'Southwest Wheatland',\n",
       "  'Southwest Wheatland'],\n",
       " 'Polonius': ['Polonius',\n",
       "  'Polonius',\n",
       "  'Polonius',\n",
       "  'Polonius',\n",
       "  'Polonius',\n",
       "  'Polonius',\n",
       "  'Polonius',\n",
       "  'Polonius',\n",
       "  'Polonius',\n",
       "  'Polonius'],\n",
       " 'Falstaff': ['Falstaff',\n",
       "  'Falstaff',\n",
       "  'Falstaff',\n",
       "  'Falstaff',\n",
       "  'Falstaff',\n",
       "  'Falstaff',\n",
       "  'Falstaff',\n",
       "  'Falstaff',\n",
       "  'Falstaff',\n",
       "  'Falstaff',\n",
       "  'Falstaff',\n",
       "  'Falstaff',\n",
       "  'Falstaff',\n",
       "  'Falstaff',\n",
       "  'Falstaff',\n",
       "  'Falstaff',\n",
       "  'Falstaff',\n",
       "  'Falstaff',\n",
       "  'Falstaff',\n",
       "  'Falstaff',\n",
       "  'Falstaff',\n",
       "  'Falstaff',\n",
       "  'Falstaff',\n",
       "  'Falstaff'],\n",
       " 'Pimlico': ['Pimlico',\n",
       "  'Pimlico',\n",
       "  'Pimlico',\n",
       "  'Pimlico',\n",
       "  'Pimlico',\n",
       "  'Pimlico',\n",
       "  'Pimlico',\n",
       "  'Pimlico'],\n",
       " 'Wheatherstone': ['Wheatherstone',\n",
       "  'Wheatherstone',\n",
       "  'Wheatherstone',\n",
       "  'Wheatherstone'],\n",
       " 'Hotspur': ['Hotspur',\n",
       "  'Hotspur',\n",
       "  'Hotspur',\n",
       "  'Hotspur',\n",
       "  'Hotspur',\n",
       "  'Hotspur',\n",
       "  'Hotspur',\n",
       "  'Hotspur',\n",
       "  'Hotspur',\n",
       "  'Hotspur',\n",
       "  'Hotspur',\n",
       "  'Hotspur',\n",
       "  'Hotspur',\n",
       "  'Hotspur',\n",
       "  'Hotspur',\n",
       "  'Hotspur',\n",
       "  'Hotspur',\n",
       "  'Hotspur',\n",
       "  'Hotspur',\n",
       "  'Hotspur',\n",
       "  'Hotspur',\n",
       "  'Hotspur',\n",
       "  'Hotspur',\n",
       "  'Hotspur'],\n",
       " 'Greco': ['El Greco', 'El Greco'],\n",
       " 'Miami': ['Southwest Miami',\n",
       "  'Southwest Miami',\n",
       "  'Southwest Miami',\n",
       "  'Southwest Miami',\n",
       "  'Southwest Miami',\n",
       "  'Southwest Miami',\n",
       "  'Southwest Miami',\n",
       "  'Southwest Miami',\n",
       "  'Southwest Miami',\n",
       "  'Southwest Miami',\n",
       "  'Southwest Miami',\n",
       "  'Southwest Miami',\n",
       "  'Southwest Miami',\n",
       "  'Southwest Miami',\n",
       "  'Southwest Miami',\n",
       "  'Southwest Miami',\n",
       "  'Southwest Miami',\n",
       "  'Southwest Miami',\n",
       "  'Southwest Miami',\n",
       "  'Southwest Miami',\n",
       "  'Southwest Miami',\n",
       "  'Southwest Miami',\n",
       "  'Southwest Miami',\n",
       "  'Southwest Miami',\n",
       "  'Southwest Miami',\n",
       "  'Southwest Miami',\n",
       "  'Southwest Miami',\n",
       "  'Southwest Miami',\n",
       "  'Southwest Miami',\n",
       "  'Southwest Miami',\n",
       "  'Southwest Miami',\n",
       "  'Southwest Miami',\n",
       "  'Southwest Miami',\n",
       "  'Southwest Miami'],\n",
       " 'Pericles': ['Pericles',\n",
       "  'Pericles',\n",
       "  'Pericles',\n",
       "  'Pericles',\n",
       "  'Pericles',\n",
       "  'Pericles',\n",
       "  'Pericles',\n",
       "  'Pericles',\n",
       "  'Pericles',\n",
       "  'Pericles',\n",
       "  'Pericles',\n",
       "  'Pericles']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audit_name(osmfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's much better and much more clear. Let's put the \"broken\" values into a dictionary so that we can manipulate them properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['212',\n",
       " '213',\n",
       " '224',\n",
       " '99E',\n",
       " 'Alley',\n",
       " 'Avenue',\n",
       " 'Boulevard',\n",
       " 'Circle',\n",
       " 'Circus',\n",
       " 'Commons',\n",
       " 'Court',\n",
       " 'Crest',\n",
       " 'Curve',\n",
       " 'Downs',\n",
       " 'Drive',\n",
       " 'East',\n",
       " 'Freeway',\n",
       " 'Grotto',\n",
       " 'Highway',\n",
       " 'Landing',\n",
       " 'Lane',\n",
       " 'Loop',\n",
       " 'North',\n",
       " 'Northbound',\n",
       " 'Parkway',\n",
       " 'Path',\n",
       " 'Place',\n",
       " 'Point',\n",
       " 'Pointe',\n",
       " 'Road',\n",
       " 'Run',\n",
       " 'South',\n",
       " 'Southbound',\n",
       " 'Street',\n",
       " 'Summit',\n",
       " 'Terrace',\n",
       " 'Trail',\n",
       " 'View',\n",
       " 'View',\n",
       " 'Vista',\n",
       " 'Way',\n",
       " 'West',\n",
       " 'Woods']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_fix = { \n",
    "                'Boticelli': 'Boticelli Street',\n",
    "                'Southwest Wheatland': 'Southwest Wheatland Road',\n",
    "                'Falstaff': 'Falstaff Road',\n",
    "                'Pimlico': 'Pimlico Drive',\n",
    "                'Hotspur': 'Hotspur Road',\n",
    "                'Southwest Miami': 'Southwest Miami Street',\n",
    "                'Pericles': 'Pericles Loop',\n",
    "                'Polonius': 'Polonius Loop',\n",
    "                'El Greco': 'El Greco Street',\n",
    "                'Wheatherstone': 'Wheatherstone Street',\n",
    "                '8202 SE Flavel St, Portland, OR 97266': 'SE Flavel Street',\n",
    "                'Cervantes': 'Cervantes Street',\n",
    "                'Touchstone': 'Touchstone Road',\n",
    "                'Polonius': 'Polonius Street',\n",
    "                'Spinosa': 'Spinosa Road',\n",
    "                'Southeast Fieldcrest': 'Southeast Fieldcrest Road'\n",
    "               }\n",
    "\n",
    "spelling_fix = { \n",
    "                'Falstaff': 'Falstaff Road',\n",
    "                'Pimlico': 'Pimlico Drive',\n",
    "                'Hotspur': 'Hotspur Road',\n",
    "                'Pericles': 'Pericles Loop',\n",
    "                'Polonius': 'Polonius Loop',\n",
    "                'El Greco': 'El Greco Street',\n",
    "                '8202 SE Flavel St, Portland, OR 97266': 'SE Flavel Street',\n",
    "                'Cervantes': 'Cervantes Street',\n",
    "                'Touchstone': 'Touchstone Road',\n",
    "                'Polonius': 'Polonius Street',\n",
    "                'Spinosa': 'Spinosa Road',\n",
    "               }\n",
    "\n",
    "# Let's sort our expected variable to aid in readability\n",
    "\n",
    "expected_values = sorted(expected_values)\n",
    "expected_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now We Will Clean The Data!\n",
    "##### (This is the best part!)\n",
    "\n",
    "The name_fix dictionary was created. Now we can actually put it to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4th Ave: 4th Avenue\n",
      "7273 SE 92nd Ave: 7273 SE 92nd Avenue\n",
      "Cervantes: Cervantes Street\n",
      "Falstaff: Falstaff Road\n",
      "Hotspur: Hostspur Road\n",
      "Pericles: Pericles Loop\n",
      "Pimlico: Pimlico Drive\n",
      "Polonius: Polonius Loop\n",
      "S Carus Rd: S Carus Road\n",
      "S Penman Rd: S Penman Road\n",
      "SE Stevens Rd: SE Stevens Road\n",
      "SE Sunnyside Rd: SE Sunnyside Road\n",
      "Southeast Hittay TRL: Southeast Hittay Trail\n",
      "Southwest Borland Rd: Southwest Borland Road\n",
      "Spinosa: Spinosa Road\n",
      "SW Boones Ferry Rd: SW Boones Ferry Road\n",
      "Touchstone: Touchstone Road\n"
     ]
    }
   ],
   "source": [
    "def update_name(name):\n",
    "    street_type = name.split(' ')[-1]\n",
    "    street_name = name.rsplit(' ', 1)[0]\n",
    "    if street_type in abbr_mapping:\n",
    "        name = street_name + ' ' + abbr_mapping[street_type]\n",
    "    elif street_type in spelling_fix:\n",
    "        if 'Falstaff' in street_name:\n",
    "            name = 'Falstaff Road'\n",
    "        elif 'Pimlico' in street_name:\n",
    "            name = 'Pimlico Drive'\n",
    "        elif 'Hotspur' in street_name:\n",
    "            name = 'Hostspur Road'\n",
    "        elif 'Pericles' in street_name:\n",
    "            name = 'Pericles Loop'\n",
    "        elif 'Polonius' in street_name:\n",
    "            name = 'Polonius Loop'\n",
    "        elif 'El Greco' in street_name:\n",
    "            name = 'El Greco Street'\n",
    "        elif '8202 SE Flavel St, Portland, OR 97266' in street_name:\n",
    "            name = 'SE Flavel Street'\n",
    "        elif 'Cervantes' in street_name:\n",
    "            name = 'Cervantes Street'\n",
    "        elif 'Touchstone' in street_name:\n",
    "            name = 'Touchstone Road'\n",
    "        elif 'Spinosa' in street_name:\n",
    "            name = 'Spinosa Road'            \n",
    "    return name    \n",
    "\n",
    "def audit_abbreviations(filename):\n",
    "    problem_street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(filename):\n",
    "        if is_street_name(elem):\n",
    "            expected_street_type(problem_street_types, elem.attrib['v'])\n",
    "    return problem_street_types\n",
    "\n",
    "def expected_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected_values:\n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "def run_updates(filename):\n",
    "    st_types = audit_abbreviations(osmfile)\n",
    "    for st_type, ways in st_types.items():\n",
    "        for name in ways:\n",
    "            better_name = update_name(name)\n",
    "            if better_name != name:\n",
    "                corrected_names[name] = better_name\n",
    "    return corrected_names\n",
    "            \n",
    "corrected_names = {}           \n",
    "corrected_names = run_updates(osmfile)\n",
    "print_sorted_dict(corrected_names, \"%s: %s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "theschema = {\n",
    "    'node': {\n",
    "        'type': 'dict',\n",
    "        'schema': {\n",
    "            'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'lat': {'required': True, 'type': 'float', 'coerce': float},\n",
    "            'lon': {'required': True, 'type': 'float', 'coerce': float},\n",
    "            'user': {'required': True, 'type': 'string'},\n",
    "            'uid': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'version': {'required': True, 'type': 'string'},\n",
    "            'changeset': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'timestamp': {'required': True, 'type': 'string'}\n",
    "        }\n",
    "    },\n",
    "    'node_tags': {\n",
    "        'type': 'list',\n",
    "        'schema': {\n",
    "            'type': 'dict',\n",
    "            'schema': {\n",
    "                'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "                'key': {'required': True, 'type': 'string'},\n",
    "                'value': {'required': True, 'type': 'string'},\n",
    "                'type': {'required': True, 'type': 'string'}\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'way': {\n",
    "        'type': 'dict',\n",
    "        'schema': {\n",
    "            'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'user': {'required': True, 'type': 'string'},\n",
    "            'uid': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'version': {'required': True, 'type': 'string'},\n",
    "            'changeset': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'timestamp': {'required': True, 'type': 'string'}\n",
    "        }\n",
    "    },\n",
    "    'way_nodes': {\n",
    "        'type': 'list',\n",
    "        'schema': {\n",
    "            'type': 'dict',\n",
    "            'schema': {\n",
    "                'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "                'node_id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "                'position': {'required': True, 'type': 'integer', 'coerce': int}\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'way_tags': {\n",
    "        'type': 'list',\n",
    "        'schema': {\n",
    "            'type': 'dict',\n",
    "            'schema': {\n",
    "                'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "                'key': {'required': True, 'type': 'string'},\n",
    "                'value': {'required': True, 'type': 'string'},\n",
    "                'type': {'required': True, 'type': 'string'}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-29939fe99b25>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    176\u001b[0m                     \u001b[0mway_tags_writer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriterows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'way_tags'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m \u001b[0mprocess_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mosmfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-20-29939fe99b25>\u001b[0m in \u001b[0;36mprocess_map\u001b[1;34m(file_in, validate)\u001b[0m\n\u001b[0;32m    166\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mel\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mvalidate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m                     \u001b[0mvalidate_element\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0melement\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'node'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-29939fe99b25>\u001b[0m in \u001b[0;36mvalidate_element\u001b[1;34m(element, validator, schema)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mvalidate_element\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melement\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtheschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;34m\"\"\"Raise ValidationError if element does not match schema\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mvalidator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melement\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m         \u001b[0mfield\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[0mmessage_string\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\\nElement of type '{0}' has the following errors:\\n{1}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\cerberus\\validator.py\u001b[0m in \u001b[0;36mvalidate\u001b[1;34m(self, document, schema, update, normalize)\u001b[0m\n\u001b[0;32m   1041\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init_processing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1043\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__normalize_mapping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocument\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1044\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfield\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocument\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\cerberus\\validator.py\u001b[0m in \u001b[0;36m__normalize_mapping\u001b[1;34m(self, mapping, schema)\u001b[0m\n\u001b[0;32m    701\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_str_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    702\u001b[0m             \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_resolve_schema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 703\u001b[1;33m         \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    704\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfield\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    705\u001b[0m             \u001b[0mschema\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfield\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_resolve_rules_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfield\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\cerberus\\schema.py\u001b[0m in \u001b[0;36mcopy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\cerberus\\schema.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, validator, schema)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidation_schema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSchemaValidationSchema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         self.schema_validator = SchemaValidator(\n\u001b[0m\u001b[0;32m     74\u001b[0m             \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[0mallow_unknown\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidation_schema\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\cerberus\\schema.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'known_rules_set_refs'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'known_schema_refs'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSchemaValidatorMixin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\cerberus\\validator.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    174\u001b[0m             \u001b[0mstructure\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m             Type: :class:`~cerberus.errors.DocumentErrorTree` \"\"\"\n\u001b[1;32m--> 176\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschema_error_tree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSchemaErrorTree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m         \"\"\" A tree representiation of encountered errors following the\n\u001b[0;32m    178\u001b[0m             \u001b[0mstructure\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\cerberus\\errors.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, errors)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 289\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparent_node\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    290\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_root\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import codecs\n",
    "import pprint\n",
    "import re\n",
    "import xml.etree.cElementTree as ET\n",
    "import cerberus\n",
    "import schema\n",
    "\n",
    "OSM_PATH = \"osmfile\"\n",
    "\n",
    "NODES_PATH = \"nodes.csv\"\n",
    "NODE_TAGS_PATH = \"nodes_tags.csv\"\n",
    "WAYS_PATH = \"ways.csv\"\n",
    "WAY_NODES_PATH = \"ways_nodes.csv\"\n",
    "WAY_TAGS_PATH = \"ways_tags.csv\"\n",
    "\n",
    "LOWER_COLON = re.compile(r'^([a-z]|_)+:([a-z]|_)+')\n",
    "PROBLEMCHARS = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "# Make sure the fields order in the csvs matches the column order in the sql table schema\n",
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    "\n",
    "def correct_element(v):\n",
    "    if v in corrected_names:\n",
    "        correct_value = corrected_names[v]\n",
    "    else:\n",
    "        correct_value = v\n",
    "    return correct_value\n",
    "\n",
    "def shape_element(element, node_attr_fields=NODE_FIELDS, way_attr_fields=WAY_FIELDS,\n",
    "                  problem_chars=PROBLEMCHARS, default_tag_type='regular'):\n",
    "    \"\"\"Clean and shape node or way XML element to Python dict\"\"\"\n",
    "\n",
    "    node_attribs = {}\n",
    "    way_attribs = {}\n",
    "    way_nodes = []\n",
    "    tags = []  # Handle secondary tags the same way for both node and way elements\n",
    "\n",
    "    if element.tag == 'node':\n",
    "        node_attribs['id'] = element.attrib['id']\n",
    "        node_attribs['user'] = element.attrib['user']\n",
    "        node_attribs['uid'] = element.attrib['uid']\n",
    "        node_attribs['version'] = element.attrib['version']\n",
    "        node_attribs['lat'] = element.attrib['lat']\n",
    "        node_attribs['lon'] = element.attrib['lon']\n",
    "        node_attribs['timestamp'] = element.attrib['timestamp']\n",
    "        node_attribs['changeset'] = element.attrib['changeset']\n",
    "        \n",
    "        for node in element:\n",
    "            tag_dict = {}\n",
    "            tag_dict['id'] = element.attrib['id']\n",
    "            if ':' in node.attrib['k']:\n",
    "                tag_dict['type'] = node.attrib['k'].split(':', 1)[0]\n",
    "                tag_dict['key'] = node.attrib['k'].split(':', 1)[-1]\n",
    "                tag_dict['value'] = correct_element(node.attrib['v'])\n",
    "            else:\n",
    "                tag_dict['type'] = 'regular'\n",
    "                tag_dict['key'] = node.attrib['k']\n",
    "                tag_dict['value'] = correct_element(node.attrib['v'])\n",
    "            tags.append(tag_dict)\n",
    "            \n",
    "    elif element.tag == 'way':\n",
    "        way_attribs['id'] = element.attrib['id']\n",
    "        way_attribs['user'] = element.attrib['user']\n",
    "        way_attribs['uid'] = element.attrib['uid']\n",
    "        way_attribs['version'] = element.attrib['version']\n",
    "        way_attribs['timestamp'] = element.attrib['timestamp']\n",
    "        way_attribs['changeset'] = element.attrib['changeset']\n",
    "        n = 0\n",
    "        for node in element:\n",
    "            if node.tag == 'nd':\n",
    "                way_dict = {}\n",
    "                way_dict['id'] = element.attrib['id']\n",
    "                way_dict['node_id'] = node.attrib['ref']\n",
    "                way_dict['position'] = n\n",
    "                n += 1\n",
    "                way_nodes.append(way_dict)\n",
    "            if node.tag == 'tag':\n",
    "                tag_dict = {}\n",
    "                tag_dict['id'] = element.attrib['id']\n",
    "                if ':' in node.attrib['k']:\n",
    "                    tag_dict['type'] = node.attrib['k'].split(':', 1)[0]\n",
    "                    tag_dict['key'] = node.attrib['k'].split(':', 1)[-1]\n",
    "                    tag_dict['value'] = correct_element(node.attrib['v'])\n",
    "                else:\n",
    "                    tag_dict['type'] = 'regular'\n",
    "                    tag_dict['key'] = node.attrib['k']\n",
    "                    tag_dict['value'] = correct_element(node.attrib['v'])\n",
    "                tags.append(tag_dict)\n",
    "    \n",
    "    if element.tag == 'node':\n",
    "        return {'node': node_attribs, 'node_tags': tags}\n",
    "    elif element.tag == 'way':\n",
    "        return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}\n",
    "\n",
    "\n",
    "# ================================================== #\n",
    "#               Helper Functions                     #\n",
    "# ================================================== #\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\"\"\"\n",
    "\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "def validate_element(element, validator, schema=theschema):\n",
    "    \"\"\"Raise ValidationError if element does not match schema\"\"\"\n",
    "    if validator.validate(element, schema) is not True:\n",
    "        field, errors = next(validator.errors.iteritems())\n",
    "        message_string = \"\\nElement of type '{0}' has the following errors:\\n{1}\"\n",
    "        error_string = pprint.pformat(errors)\n",
    "        \n",
    "        raise Exception(message_string.format(field, error_string))\n",
    "\n",
    "\n",
    "class UnicodeDictWriter(csv.DictWriter, object):\n",
    "    \"\"\"Extend csv.DictWriter to handle Unicode input\"\"\"\n",
    "\n",
    "    def writerow(self, row):\n",
    "        super(UnicodeDictWriter, self).writerow({\n",
    "            k: (v.encode('ascii') if isinstance(v, str) else v) for k, v in row.items()\n",
    "        })\n",
    "\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)\n",
    "\n",
    "\n",
    "# ================================================== #\n",
    "#               Main Function                        #\n",
    "# ================================================== #\n",
    "def process_map(file_in, validate):\n",
    "    \"\"\"Iteratively process each XML element and write to csv(s)\"\"\"\n",
    "\n",
    "    with codecs.open(NODES_PATH, 'w', encoding=\"ascii\") as nodes_file, \\\n",
    "    codecs.open(NODE_TAGS_PATH, 'w', encoding=\"ascii\") as nodes_tags_file, \\\n",
    "    codecs.open(WAYS_PATH, 'w', encoding=\"ascii\") as ways_file, \\\n",
    "    codecs.open(WAY_NODES_PATH, 'w', encoding=\"ascii\") as way_nodes_file, \\\n",
    "    codecs.open(WAY_TAGS_PATH, 'w', encoding=\"ascii\") as way_tags_file:\n",
    "\n",
    "        nodes_writer = csv.DictWriter(nodes_file, NODE_FIELDS)\n",
    "        node_tags_writer = csv.DictWriter(nodes_tags_file, NODE_TAGS_FIELDS)\n",
    "        ways_writer = csv.DictWriter(ways_file, WAY_FIELDS)\n",
    "        way_nodes_writer = csv.DictWriter(way_nodes_file, WAY_NODES_FIELDS)\n",
    "        way_tags_writer = csv.DictWriter(way_tags_file, WAY_TAGS_FIELDS)\n",
    "\n",
    "        nodes_writer.writeheader()\n",
    "        node_tags_writer.writeheader()\n",
    "        ways_writer.writeheader()\n",
    "        way_nodes_writer.writeheader()\n",
    "        way_tags_writer.writeheader()\n",
    "\n",
    "        validator = cerberus.Validator()\n",
    "\n",
    "        for element in get_element(file_in, tags=('node', 'way')):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                if validate is True:\n",
    "                    validate_element(el, validator)\n",
    "\n",
    "                if element.tag == 'node':\n",
    "                    nodes_writer.writerow(el['node'])\n",
    "                    node_tags_writer.writerows(el['node_tags'])\n",
    "                elif element.tag == 'way':\n",
    "                    ways_writer.writerow((el['way']))\n",
    "                    way_nodes_writer.writerows(el['way_nodes'])\n",
    "                    way_tags_writer.writerows(el['way_tags'])\n",
    "                    \n",
    "process_map(osmfile, validate=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
